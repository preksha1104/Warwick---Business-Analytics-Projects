---
title: "Business Statistics Mid- Term_Assessment IB94X0 2023-24 #2"
author: "5568440"
output: 
   html_document:
    toc: true
    toc_depth: 3 
editor_options: 
  chunk_output_type: console
---

---

# Declaration

This is to certify that the work I am submitting is my own. All external references and sources are clearly acknowledged and identified within the contents. I am aware of the University of Warwick regulation concerning plagiarism and collusion. 

No substantial part(s) of the work  submitted  here has also been submitted by  me  in other assessments for accredited courses of study, and I acknowledge that if this has been done an appropriate reduction in the mark I might otherwise have received will be made.

---

```{r setup, message=FALSE, warning=FALSE}

# Import Relevant Libraries

library(tidyverse) 
library(ggplot2)
library(emmeans) 
library(gridExtra)
library(Hmisc)
library(car)
library(corrplot)
library(psych)
library(lubridate)

options(width=100)
set.seed(123) # Set the random seed to make sure you get the same results

```

# Question 1

## Objectives
* The data provides no. of Bike hires from 2010 to 2023 under London Bike Hires scheme and the applicability of COVID restrictions and policies.
* Understand the effect on bike hire usage as a result of three elements of COVID response - Work from home, Rule of 6 indoors, Eat out to help out scheme.
* Exploring patterns in bike hires related to different years, month and days.
* Regression model to examine the effect of these elements upon number of bike rentals.


```{r}

Bike_hire_data <- read_csv("London_COVID_bikes.csv") # Reading the data 

```
## Data Dictionary
This data presents no. of bike hires from 2010 to 2023 categorizing for each date, day, month and year.It also includes data of COVID policies introduced and the period they were applicable. The variables are as follows:

Variable| Description
-------------| -------------
date| specific date for which data is recorded
Hires| No. of Bike hires for the given date
schools_closed| Indicates policy if schools were closed (1) or not closed (0) in response to COVID
pubs_closed| Indicates policy if pubs were closed (1) or not closed (0) in response to COVID
shops_closed| Indicates policy if shops were closed (1) or not closed (0) in response to COVID
eating_places_closed| Indicates policy if eating places were closed (1) or not closed (0) in response to COVID
stay_at_home| Indicates policy if staying at home (Lockdown) (1) or not staying home (0) in response to COVID
household_mixing_indoors_banned| Indicates policy if no one can meet people outside their household (1) or can meet people outside their household (0) in response to COVID
wfh| Indicates policy if work from home was implemented (1) or no work from home (0) in response to COVID
rule_of_6_indoors| Indicates policy to restrict indoor gatherings to no more than 6 people  (1) or not restrict indoor gatherings to no more than 6 people (0) in response to COVID
curfew| Indicates policy to restrict movement and activities during specific hour of day (1) or not restrict movement and activities during specific hour of day (0) in response to COVID
eat_out_to_help_out| Indicates policy to encourage people to dine in restaurants and cafe and by giving discounts (1) or not give discounts for dine (0) in response to COVID
day| day of the recorded date
month| month of the recorded date
year| year of the recorded date

## Data integrity check and cleaning

```{r}
# Check the structure, missing values and duplicates in data

str(Bike_hire_data)  # cross check structure of data

summary(Bike_hire_data) # check the summary of data

sum(is.na(Bike_hire_data)) # check no missing value

duplicates_in_Bike_hire_data <- duplicated(Bike_hire_data)
summary(duplicates_in_Bike_hire_data)  # no duplicates in data set


# convert day into factor with levels

day_levels = c("Mon","Tue","Wed","Thu","Fri","Sat","Sun")
Bike_hire_data <- Bike_hire_data %>%
  mutate(day = factor(day, levels = day_levels)) 

# convert month into factor with levels

month_levels = c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec")
Bike_hire_data <- Bike_hire_data %>%
  mutate(month = factor(month, levels = month_levels))

# convert year into factor

Bike_hire_data$year <- as.factor(Bike_hire_data$year)

str(Bike_hire_data)
summary(Bike_hire_data)

```
Based on summary,we can note that minimum and maximum for 2010 and 2023 is not for complete year.The mean no. of hires are 26,607.
we can see that Work from home policy is being applied to maximum number of days compared to all other policies comparing mean of all policies.


```{r}
# Check any outliers in data

ggplot(Bike_hire_data, aes(x= year, y= Hires)) + geom_boxplot(outlier.color = "blue")+ labs(
    x = "Year",
    y = "Number of Bike Hires",
    title = "Distribution of Bike Hires Over Years")+   theme_minimal() 

# Check how removal of outliers impact the meean no. of hires

threshold <-1.5
Bike_hire_data_no_outliers <- Bike_hire_data %>%
  group_by(year) %>%
  filter(Hires >= quantile(Hires, 0.25) - threshold * IQR(Hires),
         Hires <= quantile(Hires, 0.75) + threshold * IQR(Hires))

# comparing mean with and without outliers

(mean_with_outliers<- mean(Bike_hire_data$Hires))

(mean_without_outliers<- mean(Bike_hire_data_no_outliers$Hires))

# filter the records with no. of hires 0

(zero_hires_2022 <- Bike_hire_data %>%
  filter(year(date) == 2022, Hires == 0) %>%
  select(date))

# removing the records with hires 0

Bike_hire_data_revised <- Bike_hire_data %>%
  filter(Hires != 0)

```

Based on understanding, we can notice there are a few outliers at threshold of 1.5 but removing them does not significantly affect the mean no. of bike hires. They can be due to specific events like covid in 2020 so removing them might lead to valuable data loss. It will provide a good representation to build a robust model

There are 2 days with zero hires because santander cycles were out of action for that weekend on 10th and 11th september 2022, we can remove them as the goal of our analysis is focused on user behaviour in response to covid policies during operational period. (Article Ref 1)



## Trend of no. of hires over the years

```{r}

Bike_hire_data_revised %>%
  ggplot(aes(x = month, y = Hires, fill = as.factor(month))) + # compare hires month wise for each year
  geom_bar(stat = "summary", fun = "mean", position = position_dodge(width = 0.8), alpha = 0.7) + # show mean values of bike hire for each month
  labs(x = "Month", y = "Mean Hires", fill = "Month", title = "1.1 Bike Hires Across Months (Grouped by Years)") + #add labels
  theme_minimal() + # add theme
  facet_wrap(~ year, scales = "free_y", ncol = 3) + # split graph for each year
  theme(
    axis.text.x = element_text(angle = 90, hjust = 1),  # Rotate x-axis labels
    legend.position = "right"  # Place the legend at the top
  )


```


We can note following things from above graph 1.1:

1. Data in for first 6 month in 2010 and last 3 month in 2023 is not provided in dataset as no mean is reflecting in graph
2. The trend generally shows the number of bike hires are more during spring and summer seasons ( April to June) and lowest in winter season (November to february) explaining the customer behvaiour and effect of seasonality. (excluding 2010 and 2023)
3. Some unsual trends can be observed in 2020 and 2021 due to covid and in response to policies introduced, which we can analyse more in detail.

```{r}
# removing 2010 and 2023 from dataset before further analysis

Bike_hire_data_revised <- Bike_hire_data_revised %>%
  filter(!(year %in% c("2010", "2023")))

# creating a column covid_policies if any policy is applicable

Bike_hire_data_all <- Bike_hire_data_revised %>%
  mutate(covid_policies = if_else(wfh + rule_of_6_indoors + eat_out_to_help_out+schools_closed+pubs_closed+shops_closed+eating_places_closed+stay_at_home+household_mixing_indoors_banned+curfew > 0, 1, 0))
  
# convert covid_policies into factor

Bike_hire_data_all$covid_policies <- as.factor(Bike_hire_data_all$covid_policies)

```

## Trend analysis using graphical presentation

```{r}
Bike_hire_data_all %>%
  group_by(month, year) %>%
  ggplot(aes(x = year, y = Hires, group = interaction(month, year), fill = factor(year))) + #compare months for each year
  geom_col(position = position_dodge(width = 0.6), alpha = 0.3)  + #type of graph
  labs(x = "Year", y = "Mean Hires", fill = "Year", title = "1.2 Bike Hires Across Years (Grouped by Month)") + # add labels
  theme_minimal() + # add theme
  facet_wrap(~ month, ncol = 3) + # each month shown separateky
  theme(
    axis.text.x = element_text(angle = 90, hjust = 1),  # Rotate x-axis labels
    legend.position = "right"  # Place the legend at the top
  )
```

We can note following things from above graph 1.2:

We can observe that the no. of hires are generally similar in each month across years, other than few exceptions which can be noticed where we have faded area in the bar graph

COVID hit in Mar 2020
March 2020 is the least in march as covid 1st wave started in march 2020

We can see a huge rise in May 2020 to Aug 2020 which was during covid period can be due to people being afraid to use public transport or want to work towards fitness in free time due to being home. (Article Ref 2)



```{r}

# Trend from 2019 to 2023 for no. of Bike Hires 

Bike_hire_data_all_subset <- Bike_hire_data %>%
  filter(date >= as.Date("2019-01-01") & date <= as.Date("2023-12-01")) %>% # make a data subset
  group_by(year, month)

# Create a time series plot using ggplot2
ggplot(Bike_hire_data_all_subset, aes(x = date, y = Hires)) +
  geom_line(color = "black", size = 0.5) + geom_smooth(aes(y = Hires), color = "blue", size = 1, method = "loess") + # comparsion of date and no. of hires
  labs(title = "1.3 Bike Hires Over Time (2019-2022)",
       x = "Time",
       y = "Number of Bike Hires") + # add labels
  theme_minimal() # add theme

```

We can note following things from above graph 1.3:

We can substantiate the upward trend in the number of bike hires during the COVID period as compared to the non-COVID period (2019), particularly observed from May to August 2020. 

Following this, an overall increasing trend is noticeable, influenced by seasonal variations that depict fluctuations corresponding to summer and winter seasons. 

Post-2022, there is a slight decreasing trend in the number of bike hires. Consequently, it can be asserted that the COVID-19 period had a positive impact on bike hires.

```{r}

# Impact on no. of bike hires on weekdays and weekend

Bike_hire_data_all %>%
  group_by(day, year) %>%
  ggplot(aes(x = day, y = Hires, group = interaction(day, year), fill = factor(year))) + #compare days for each year
  geom_col(position = position_dodge(width = 0.6), alpha = 0.7)  + # type of graph
  labs(x = "Days", y = "Mean Hires", fill = "Year", title = "1.4 Bike Hires Across Days (Grouped by Year)") + # add labels
  theme_minimal() + # add theme
  facet_wrap(~ year, ncol = 3) + # different graph for each year
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate x-axis labels
    legend.position = "right"  # Place the legend at the top
  )

```

We can note following things from above graph 1.4:

Observing the graph, it becomes apparent that from 2017 to 2019, the number of bike hires on weekends consistently lagged behind that of weekdays. However, in the COVID-19 period spanning 2021 and 2022, a noticeable shift occurred, with weekends experiencing the highest bike hires. As a result, the graph suggests that the percentage increase in weekend bike hires in 2020, relative to the pre-COVID era, is likely more substantial than the increase observed on weekdays. (Article Ref 2)

```{r}

# Group by 'month' and 'covid_policies', and calculate the mean for 'Hires'
monthly_means <- Bike_hire_data_all %>%
  group_by(month, covid_policies) %>%
  summarise(mean_hires = mean(Hires))

# Group by 'day' and 'covid_policies', and calculate the mean for 'Hires'
daily_means <- Bike_hire_data_all %>%
  group_by(day, covid_policies) %>%
  summarise(mean_hires = mean(Hires))



# Effect of Covid policies on month and day graphically

grid.arrange(ggplot(monthly_means, aes(x = month, y = mean_hires, group = covid_policies, color = factor(covid_policies))) + # monthly comparison
  geom_line() + # graph type
  labs(title = "1.5 Monthly Trend of Bike Hires",
       x = "Month",
       y = "Mean Hires",
       color = "Covid Policies") + # add labels
  scale_x_discrete(labels = month.abb) +  # Use abbreviated month names on the x-axis
  theme_minimal(), ggplot(daily_means, aes(x = day, y = mean_hires, group = covid_policies, color = factor(covid_policies))) + # daily comparsion
 geom_line()+ #graph type
  labs(title = "1.5 Daily Trend of Bike Hires",
       x = "Day",
       y = "Mean Hires",
       color = "Covid Policies") + #add labels
  theme_minimal(), ncol = 2)


```

Based on the visualizations 1.5, we can assert that the implementation of any of the COVID policies, indicated by a value of 1:

The number of bike hires tends to be higher from February to August, after which it stabilizes and becomes nearly equivalent to the period when no COVID policies were in effect.

There is an increased number of bike hires from Friday to Sunday, with the most significant rise observed on Saturday and Sunday, as indicated by the noticeable difference between the lines.

## Correlation between variables

```{r}

all.policies.effect <- lm(Hires ~ wfh + rule_of_6_indoors + eat_out_to_help_out+schools_closed+pubs_closed+shops_closed+eating_places_closed+stay_at_home+household_mixing_indoors_banned+curfew, data = Bike_hire_data_all) # multiple regression for all policies effect of hires

summary(all.policies.effect) # summary of lm

#vif(all.policies.effect) # showing error because multicollinearity exist


```
The above model provides relationship between Hires and various independent policies effect on bike hires. When there is no policies the intercept is 26,640 hires (p <0.001). However effect of few policies where p >0.05 is not statistically significant like schools closed, pubs closed, shops closed, household missing indoors banned which indicates that there may be issue of multicollinearity.


When the Variance Inflation Factor (VIF) is calculated, and it indicates "there are aliased coefficients in the model," it signifies the presence of perfect multicollinearity among the predictors in the regression model. This situation suggests the need to simplify the model by removing independent variables with high correlations. Such multicollinearity can distort the model and introduce errors.


```{r} 
#visual presentation of correlation where policies are applicable

Bike_hire_data_all_policies <-Bike_hire_data_all %>%
  subset(Bike_hire_data_all$covid_policies == "1") # segregate the data where any of the poilicies are applicable to check the relationship


correlation_matrix <- cor(Bike_hire_data_all_policies[,sapply(Bike_hire_data_all_policies, is.numeric)])

corrplot(
  correlation_matrix,
  method = "color",   # Add squares
  type = "upper",      # Display in the upper triangle
  tl.cex = 0.5,        # Adjust text size for numbers
  addCoef.col = "black",  # Color of the numbers
  diag = FALSE          # Exclude diagonal
)


```

We concentrated our analysis on the period when any COVID policies were in place (where covid_policies = 1) to eliminate any biases

In certain policies like schools_closed, pubs_closed, shops_closed, eating_places_closed, stay_at_home, and household_mixing_indoors_banned, the dark blue color and high positive correlation values suggest a strong connection between different factors/policies. This situation, known as database multicollinearity, can make our model less reliable and less likely to show statistically significant results.

For the 'Curfew' variable, we observed a very small correlation (r value = -0.07) with bike hires. This indicates that the restrictions on outdoor activities during curfew hours don't have a substantial impact on bike hires.

Therefore, to avoid complications from multicollinearity, we've decided to focus our further analysis on a subset of three policies: 'Rule of 6 Indoor,' 'Work from Home,' and 'Eat Out to Help Out.' These policies also show a weak correlation with each other, making our analysis more robust and interpretable.

```{r}

#Finalising dataset for modeling and making all the variables as factors.

Bike_Hires_data_final <- Bike_hire_data_all%>% 
  select("date","Hires","rule_of_6_indoors","eat_out_to_help_out","wfh","day","month","year")

# adding a column covid policies where any of the three policies are applicable

Bike_Hires_data_final <- Bike_Hires_data_final %>%
  mutate(covid_policies = if_else(wfh + rule_of_6_indoors + eat_out_to_help_out> 0, 1, 0))

# converting all into factor

Bike_Hires_data_final$wfh <- as.factor(Bike_Hires_data_final$wfh)
Bike_Hires_data_final$rule_of_6_indoors <- as.factor(Bike_Hires_data_final$rule_of_6_indoors)
Bike_Hires_data_final$eat_out_to_help_out <- as.factor(Bike_Hires_data_final$eat_out_to_help_out)
Bike_Hires_data_final$covid_policies <- as.factor(Bike_Hires_data_final$covid_policies)

str(Bike_Hires_data_final)
```

## Impact of Policies on bike hires

```{r}
# impact of wfh policy on hire

wfh.policy <- lm(Hires ~ wfh, data = Bike_Hires_data_final)

summary(wfh.policy) # summary of lm

cbind(coefficient=coef(wfh.policy), confint(wfh.policy)) # CI of lm

```

The estimated intercept at no policy is estimated 26733 bike hires [26417 - 27049]. It explains increase in no. of hires by 2572 [1841.832 - 3303.812] when wfh policy is implemented both have very small p value < 0.001 with 95% confidence interval.The standard error of 9622 seems on higher side and multiple r squared is 1.075%.
The model suggests that the relationship is statistically significant but overall R square is relatively low and CI for wfh being too wide which suggest it may not have strong predictive power on its own.

```{r}
# impact of rule_of_6_indoors policy on hire

rule.6.indoor.policy <- lm(Hires ~ rule_of_6_indoors, data = Bike_Hires_data_final)

summary(rule.6.indoor.policy) # summary of lm

cbind(coefficient=coef(rule.6.indoor.policy), confint(rule.6.indoor.policy)) # CI of lm

```
The estimated intercept at no policy is estimated 27023 bike hires [26735 - 27310]. It explains increase in no. of hires by 8708 [6768 - 10648] when rule of 6 indoor policy is implemented both have very small p value < 0.001 with 95% confidence interval.The standard error of 9590 seems on higher side and multiple r squared is 1.737%.
The model suggests that the relationship is statistically significant but overall R square is relatively low and CI for rule of 6 indoors being too wide which suggest it may not have strong predictive power on its own.

```{r}
# impact of eat_out_to_help_out policy on hire

eat.help.out.policy <- lm(Hires ~ eat_out_to_help_out, data = Bike_Hires_data_final)

summary(eat.help.out.policy) # summary of lm

cbind(coefficient=coef(eat.help.out.policy), confint(eat.help.out.policy)) #CI of lm

```
The estimated intercept at no policy is estimated 27154 bike hires [26868 - 27441]. It explains increase in no. of hires by 9263 [5678 - 12848] when eat out to help out policy is implemented both have very small p value < 0.001 with 95% confidence interval.The standard error of 9646 seems on higher side and multiple r squared is 0.5824%.
The model suggests that the relationship is statistically significant but overall R square is relatively very low and CI for eat out to help out being too wide which suggest it may not have strong predictive power on its own.

Lets investigate impact of all three policies as a multiple regression

```{r}

# impact of all three policy on hires

all.3.policy.add<-lm(Hires ~ wfh+rule_of_6_indoors + eat_out_to_help_out, data = Bike_Hires_data_final)

summary(all.3.policy.add) #summary of lm

confint(all.3.policy.add) # CI of lm

vif(all.3.policy.add) # check vif score

```
The estimated intercept at no policies is 26639.7  [26325.181, 26954.153] bike hires.The estimated increase in the number of hires when working from home (wfh) is implemented is 1873.9 [1118.691, 2629.148]. The estimated increase in the number of hires when the rule of 6 indoors is implemented is 7373.8 [5363.934, 9383.753].The estimated increase in the number of hires when the eat_out_to_help_out policy is implemented is 9778.8 [6231.563, 13325.960] at 95% confidence interval.The p-values for all coefficients are very small, less than 0.001, indicating that the relationships are statistically significant. The standard error of 9536 is relatively high. 
The multiple R-squared is 0.02869, indicating that only a small proportion (2.869%) of the variance in Hires is explained by these policies which may suggest there are other important factor not considered.

The vif score scores are less than 5 so model does not have any issue of multicollinearity.

Also, The confidence intervals for the coefficients are relatively wide, indicating uncertainty in the estimates.

Lets try interaction term model to compare

```{r}

# impact of all three policy on hires and intercation terms

all.3.policy.intr<-lm(Hires ~ wfh*rule_of_6_indoors*eat_out_to_help_out, data = Bike_Hires_data_final)

summary(all.3.policy.intr) #summary of lm

confint(all.3.policy.intr) # summary of CI

vif(all.3.policy.intr, type = 'predictor') # check vif score

```

The coefficient for "wfh1:rule_of_6_indoors1" is -9520.8 [-2580, -16461] at 95% CI with p value  0.00719. It shows that impact of wfh hires is reduced when rule of 6 indoors is implemented.

The coefficient of other interaction term is na due to singularities.

The gvif score is 1 so there is no severe multicollinearity issue in model

This model has residual error of 9530 which is relatively large, complex  and low adjusted R-squared value - 2.9%.

```{r}
# comparison of models showing individual policy impact

anova(wfh.policy,rule.6.indoor.policy,eat.help.out.policy)
```
The p value is >0.05, hence the models do not significantly differ in their ability to explain the variance in bike hires based on the given predictors.

```{r}
# comparison of models showing all policy and all policy with intercation term

anova(all.3.policy.add,all.3.policy.intr)
```
The F-statistic is 7.2323, and the associated p-value < 0.01 suggesting that Model 2, which includes interaction terms, provides a statistically significant improvement in explaining the variance in bike hires compared to the simpler Model 1.

The above analysis assesses the individual impacts of working from home, the rule of 6 indoors, and eat out to help out policies on bike hires. While each policy shows a statistically significant effect, the multiple regression and interaction term models exhibit low predictive power with wide confidence intervals and high residual errors. The models suggest that other factors not considered in the analysis may play a significant role in influencing bike hires. 

We can consider the effect of time variables on no. of bike hires to explain the variability more.

## Impact of time (day, month, year) on bike hires

```{r}

#analysis of effect of month on Hires.
month.all <- lm(Hires ~month, data = Bike_Hires_data_final)
summary(month.all) # summary of lm
confint(month.all) #CI of lm
```
The overall model is statistically significant, as indicated by the extremely low p-value (< 2.2e-16) of the F-statistic (262.9 on 11 and 4370 DF). The multiple R-squared value is 0.3982, indicating that approximately 39.82% of the variance in bike hires is explained by the model.

The month of december with p value = 0.13619 states that effect of December on bike hires is not statistically significant.

There is a noticeable seasonal trend increase  as March, April, May, June, July, August, and September experience subtantial increase in estimated bike hires, as during spring and summer season outdoor activities like biking will be more prevalent.

```{r}
#analysis of effect of days on Hires.

day.all <- lm(Hires ~day, data = Bike_Hires_data_final)
summary(day.all) #summary of lm
confint(day.all) #CI of lm

```
The overall model is statistically significant, as indicated by the extremely low p-value (< 2.2e-16) of the F-statistic (34.75 on 6 and 4375 DF). The multiple R-squared value is 0.04549, indicating that approximately 4.6% of the variance in bike hires is explained by the model. The model does not explain much alone about trends and overall shows a decreasing trend on weekends compared to weekdays. The resideual standard error of 9457 states larger average prediction error, indicating potential limitations in capturing variability only based on days.

```{r}
#analysis of effect of year on Hires.
year.all <- lm(Hires ~ year, data = Bike_Hires_data_final)
summary(year.all) # summary of lm
confint(year.all) #CI of lm

```
The overall model is statistically significant, as indicated by the extremely low p-value (< 2.2e-16) of the F-statistic (49.17 on 11 and 4370 DF). The multiple R-squared value is 0.1101, indicating that approximately 11.01% of the variance in bike hires is explained by the model.

The residual standard error of 9136, indicating the variability in bike hires that is not explained by the "year" variable. 

Overall, the coefficient suggests a positive and substantial growth in the number of bike hires over the year from 2012 to 2022.

```{r}
# comparsion of models considering day, month and year individually

anova(day.all,month.all, year.all)

```
The results indicate a significant difference in the fit between the models. Model 2 (Hires ~ month) has a substantially lower residual sum of squares compared to Model 1 (Hires ~ day), as evidenced by the significant F-statistic and very low p-value. This suggests that the inclusion of month as a predictor significantly improves the model's ability to explain the variance in bike hires compared to using day alone concluding rejection of Null hypothesis.

## Impact of day, month and year with covid policies

```{r}
#analysis of effect of applicability of covid policies, day, month and year on Hires.

time.policies.all <- lm(Hires ~ covid_policies+ day+ month + year, data = Bike_Hires_data_final)


summary(time.policies.all) #summary of lm

vif(time.policies.all) # check vif score

```

Covid_Policies variable is created basis, if any of the 3 policies is applicable it will be 1, otherwise 0. This will help us check the impact of policies with time variables.

The overall model is statistically significant, as indicated by the extremely low p-value (< 2.2e-16) of the F-statistic (191.5 on 29 and 4370 DF). The adjusted R-squared value is 0.5578, indicating that approximately 55.78% of the variance in bike hires is explained by the model. The residual standard error rate is also the least among all the above model. 




```{r}
# lets examine the results of days with applicability of covid policies as the R square was lowest in above model

day.covid.policies<- lm(Hires~ covid_policies*day, data = Bike_Hires_data_final)
summary(day.covid.policies) # summary of lm

vif(day.covid.policies) # check vif score
```
The overall model is statistically significant, as indicated by the extremely low p-value (< 2.2e-16) of the F-statistic (31.46 on 13 and 4368 DF). The adjuested R-squared value is 0.0829, indicating that approximately 8.29% of the variance in bike hires is explained by the model. Due to high vif score we should be caution when drawing conclusions.

The interaction term of covid policies applicable on a weekend are statistically significant as they have small p - value < 0.001. We can notice a difference in trend that when covid policies are applicable there is rise in no. of bike hires on weekend as compared to fall in no. of bike hires in the earlier regression model only comparing days. This is a more generalised model as it deals with more variability.

```{r}
# lets examine the results of month with applicability of covid policies if it gives more insight

month.covid.policies<- lm(Hires~ covid_policies*month, data = Bike_Hires_data_final)
summary(month.covid.policies) # summary of lm

vif(month.covid.policies) # check vif score
```
The overall model is statistically significant, as indicated by the extremely low p-value (< 2.2e-16) of the F-statistic (140.1 on 23 and 4358 DF). The adjuested R-squared value is 0.422, indicating that approximately 42.2% of the variance in bike hires is explained by the model. Due to high vif score we should be caution when drawing conclusions.

The interaction term of covid policies applicable on a month are statistically significant from May to august as they have small p - value < 0.001. We can notice that no. of bike hires increase in these months due to seasonality as discussed earlier. The most increase is in month of April.

## Graphical presentation of interaction using estimation approach

```{r}
# these models have high vif and should be concluded caustionaly , we are just presenting graphically to check the effect of time variables when covid policies are in action.

month.all.emm <- emmeans(month.all, ~month  )
day.all.emm<- emmeans(day.all, ~day)
year.all.emm <- emmeans(year.all, ~year)

day.covid.policies.emm <- emmeans(day.covid.policies, ~day)
month.covid.policies.emm <- emmeans(month.covid.policies, ~month)

year.covid.policies<- lm(Hires~ covid_policies*year, data = Bike_Hires_data_final)
year.covid.policies.emm <- emmeans(year.covid.policies, ~year)


year.all.emm.summary <- summary(year.all.emm)
year.all.emm.summary$model <- "Only Years"

year.covid.policies.emm.summary <- summary(year.covid.policies.emm)
year.covid.policies.emm.summary$model <- "Year with covid policies"

year.summary <- rbind(year.all.emm.summary, year.covid.policies.emm.summary)

ggplot(year.summary, aes(x=year, y=emmean, ymin=lower.CL, ymax=upper.CL, col=model)) + geom_point() + geom_linerange() + labs(x="Years", y="Hires", col="Model", title = "Effect of covid policies across Years")

```

We can see graphically that the mean of hires is impacted by covid policies in 2020 and 2021. Also we can see a rising trend in no. of bike hires due to covid policies in 2021 due to awareness for fitness after covid and use of less public transport by people.

```{r}

month.all.emm.summary <- summary(month.all.emm)
month.all.emm.summary$model <- "Only Month"

month.covid.policies.emm.summary <- summary(month.covid.policies.emm)
month.covid.policies.emm.summary$model <- "Months with covid policies"

month.summary <- rbind(month.all.emm.summary, month.covid.policies.emm.summary)

ggplot(month.summary, aes(x=month, y=emmean, ymin=lower.CL, ymax=upper.CL, col=model)) + geom_point() + geom_linerange() + labs(x="Month", y="Hires", col="Model",title = "Effect of covid policies across Months")

```

We can see graphically that there is rise in bike hires due to seasonality, also the increase is more during period where covid policies are applicable in month Feb to august. In other months, there is not much noticebale change in bike hires additionally due to covid policies.

```{r}

day.all.emm.summary <- summary(day.all.emm)
day.all.emm.summary$model <- "Only Days"

day.covid.policies.emm.summary <- summary(day.covid.policies.emm)
day.covid.policies.emm.summary$model <- "Days with covid policies"

day.summary <- rbind(day.all.emm.summary, day.covid.policies.emm.summary)

ggplot(day.summary, aes(x=day, y=emmean, ymin=lower.CL, ymax=upper.CL, col=model)) + geom_point() + geom_linerange() + labs(x="Day", y="Hires", col="Model",title = "Effect of covid policies across Days of Week")


```
We can see graphically that during weekday there is not much difference in mean bike hires when covid policies are applicable. But on weekends, in case of no covid policies the no. of hires reduces but when covid polcies are applicable we can see a huge jump in no. of bike hires.

## Conclusion

1. There is consistent seasonal trend observed by hike in no. bike hires during spring and summer season when considering individual years.
2. While individual policies (Work from Home, Rule of 6 Indoors, and Eat Out to Help Out) show statistically significant effects on bike hires but its better when interpreted with time variables to conclude the effects of policies.
3. During the COVID period, weekends experience higher bike hires compared to weekdays. 
4. The regression model incorporating COVID-19 policies, day, month, and year appears suitable with simplicity for interpretability. Rigorous evaluation, including statistical significance, VIF analysis, and model fit, supports its appropriateness for understanding the dynamics of bike hires during the pandemic.

# Question 2

## Objectives
* The data provided contains information on e-book sales over a period of many months each row represents one book. 
* Impact of Total no of reviews and average review score on sales for a book.
* Effect of Sales price on sales across different genre.


```{r}
book_sales <-read_csv("publisher_sales.csv") # Reading the data 
```

## Data Dictionary
This data presents e-book sales data where each row presents unique book. The variables are as follows:

Variable| Description
-------------| -------------
sold by| Identity responsible for selling book
publisher.type| category of publisher of the book
genre| type of book like adult fiction, YA fiction and non-fiction 
avg.review| average review score of each book ranging from 0 to 5
daily.sales| average number of books sold (Less refunds) across all days of the period 
total.reviews| total number of reviews received by the book
sale.price| average price of book sold in period

## Data integrity check and cleaning


```{r}
# Check the structure, missing values and duplicates in data

str(book_sales) # cross check structure of data

summary(book_sales) # check the summary of data, no missing value

duplicates_in_book_sales <- duplicated(book_sales)
summary(duplicates_in_book_sales) # no duplicates in data set


```

Based on summary, majority books are sold by Amazon even though least published by them as indie publisher and single author also sells through amazon. The majority of books have high review score/ ratings as is evident based on mean of average review score being 4.269. 


```{r}
# Check any outliers in review score and total no. of review

ggplot(book_sales, aes(x= avg.review))+ geom_histogram(binwidth= 0.1) # there are very few books with no review score

ggplot(book_sales, aes(x= total.reviews))+ geom_histogram(binwidth= 1) # the books with no review score also has no total reviews 

book_sales_review <- book_sales[book_sales$total.reviews >0,]

summary(pull(book_sales_review,total.reviews))
summary(pull(book_sales_review,avg.review))# removing books with 0 reviews and checking summary


```

There are no extreme outliers in average review score and total reviews, there are 23 entries with 0 review score which does not affect the mean and explains the natural variation not caused due to error or anomalies. Cases with no review for books will lead to no review score but can have sales which we should consider going forward in analysis.


```{r}
# Check any outliers in daily sales

ggplot(book_sales, aes(x= daily.sales))+ geom_histogram(binwidth= 1) # to understand distribution

ggplot(book_sales, aes(y= daily.sales))+ geom_boxplot() # to understand outliers

# removal of outliers in daily sales using threshold method

threshold <- 1.5
book_sales_revised <- book_sales %>%
  group_by(genre) %>%
  filter(daily.sales >= quantile(daily.sales, 0.25) - threshold * IQR(daily.sales),
         daily.sales <= quantile(daily.sales, 0.75) + threshold * IQR(daily.sales)) # removal of outliers 

ggplot(book_sales_revised, aes(y= daily.sales))+ geom_boxplot() # recheck after removal
```

The daily sales exhibits instances characterized by extreme values surpassing the 1.5 interquartile range threshold. Removing outliers will enhance the clarity and interpretability of visualizations.Given the modest size of the dataset, the removal of outliers is expected to result in minimal loss of information.


```{r}
# Check any outliers in sale price

ggplot(book_sales_revised, aes(x= sale.price))+ geom_histogram(binwidth= 1) # to understand distribution

```

The sales price seem to have a normal distribution and no outliers can be spotted

## Trend and relationship analysis between variables

```{r}
# understand the distribution of daily sales across genre and comparison with the mean of sales across genre

ggplot(book_sales_revised, aes(x = daily.sales, color = genre)) +
  geom_density( alpha = 0.5) + # show the distribution genre wise
  stat_function(fun = dnorm, args = list(mean = mean(book_sales_revised$daily.sales), sd = sd(book_sales_revised$daily.sales)), color = "black", linetype = "dotdash", size = 1) +  # show the distribution overall
 geom_vline(aes(xintercept = mean(daily.sales)), color = "black", linetype = "solid", size = 1.5) + #show the mean books sold overall
  ggtitle("2.1 Distribution of Daily Sales - Genre wise") +
 xlab("Daily Sales")+
  ylab("Density")+theme_grey() #labels and set theme

```

Graph 1.1 illustrates that the average distribution of YA_fiction surpasses the overall genre average, while adult fiction remains relatively consistent and non-fiction exhibits a lower average distribution. This implies that, on average, YA_fiction tends to outperform other genres in terms of book sold, while non-fiction lags behind.

 
```{r}
# understand relationship between total no. of review and average review score across genre

ggplot(book_sales_revised, aes(x = avg.review, y = total.reviews, color = genre)) + geom_point(alpha = 0.2) + # show the relationship genre wise
   geom_smooth(method = "lm", se = FALSE, linetype = "dashed", color = "black") + # adding overall relation line
    geom_smooth(method = "lm", se = FALSE, linetype = "solid") +  # Adding individual genre line
  labs(x = "Ratings", y = "Total no. of  Reviews", title = "2.2 Genre-wise and Overall: Ratings vs. Total Reviews") + # add labels and title
  theme_minimal() # set theme
```


In Graph 1.2, we observe a subtle yet positive correlation between total reviews and ratings, both in the overall dataset and when examined by genre. This indicates that individuals who take the time to provide reviews generally assign higher ratings. Notably, the YA_fiction genre stands out, displaying the highest number of reviews associated with ratings surpassing 4. This also states that reviewers are more inclined to express positive ratings which has elevated the overall average rating.


```{r}

# check the average rating score genre wise

average_ratings <- book_sales_revised %>%
  group_by(genre) %>%
  summarise(mean_rating = mean(avg.review, na.rm = TRUE))%>% mutate(mean_rating = round(mean_rating, 2)) # calculate the mean rating

average_ratings

```

As we can see that genre has a limited impact on overall book ratings.


```{r}

# understand relationship between average rating and daily sales

ggplot(book_sales_revised, aes(x = avg.review, y = daily.sales, color = genre)) + geom_point(alpha = 0.2)+ # avg review comparison genre wise
  geom_smooth(method = "glm", se = FALSE, linetype = "dashed", color = "black") +
  geom_smooth(method = "glm", linetype = "solid") +# relationship
  labs(x = "Ratings", y = "Daily Sales", title = "2.3 Genre-wise and Overall: Ratings vs. Daily Sales") + # add labels
  theme_minimal() # add theme

```

In Graph 1.3, it is evident that there is minimal correlation between ratings and the number of books sold across genres. The consistent level of book sales, regardless of ratings falling between 4 to 5, suggests that variations in ratings do not significantly impact the quantity of books sold.

```{r}

# understand relationship between total ratings and daily sales

ggplot(book_sales_revised, aes(x = total.reviews, y = daily.sales, color = genre)) + geom_point(alpha = 0.2)+
  geom_smooth(method = "glm", se = FALSE, linetype = "dashed", color = "black") +
  geom_smooth(method = "glm", linetype = "solid") +
  labs(x = "Total reviews", y = "Daily Sales", title = "2.4 Genre-wise and Overall: Total reviews vs. Daily Sales") +
  theme_minimal() 

```

In Graph 1.4, a positive correlation is evident between total reviews and the number of books sold across various genres. This suggests that a higher volume of reviews for a book is associated with increased sales, indicating a potential link between reviews and book popularity.

```{r}
# understand relationship between publisher and total reviews

average_no._of_reviews <- book_sales_revised %>%
  group_by(publisher.type) %>%
  summarise(mean_reviews = mean(total.reviews, na.rm = TRUE))%>% mutate(mean_reviews = round(mean_reviews, 0))

average_no._of_reviews
```

The provided summary suggests that the average number of reviews remains consistent across books sold through different publisher types. Consequently, the choice of publisher type does not appear to have a significant impact on the number of reviews received and ratings by readers.

```{r}
# understand the distribution of sale price across genre and comparison with the mean of sale price across genre

ggplot(book_sales_revised, aes(x = sale.price, color = genre)) +
  geom_density( alpha = 0.5) + # density plot genre wise
  stat_function(fun = dnorm, args = list(mean = mean(book_sales_revised$sale.price), sd = sd(book_sales_revised$sale.price)), color = "black", linetype = "dotdash", size = 1) + # overall distribution
 geom_vline(aes(xintercept = mean(sale.price)), color = "black", linetype = "solid", size = 1.5) + # mean overall data
  ggtitle("2.5 Distribution of Sales price - Genre wise") +
 xlab("Sales Price")+ 
  ylab("Density")+theme_grey() # add labels and theme

```

In Graph 1.5, it's clear that the average prices for YA fiction and adult fiction are relatively narrower than the mean sales price, while non-fiction stands at the higher end of the spectrum. 
This pricing pattern aligns with the previous observation in Graph 1.1, where YA fiction outperforms in sales, adult fiction maintains consistency, and non-fiction lags behind, suggesting a potential correlation between pricing, genre, and sales performance.


```{r}
# understand relationship between sale price and daily sales genre wise to see further trend based on above observation

ggplot(book_sales_revised, aes(x = sale.price, y = daily.sales, color = genre)) + geom_point(alpha = 0.2)+ # plotting genre wise
  geom_smooth(method = "glm", linetype = "solid") +  geom_smooth(method = "glm", se = FALSE, linetype = "dashed", color = "black")+ # overall data line
  labs(x = "sale price", y = "Daily Sales", title = "2.6 Genre-wise and Overall: Sale price vs. Daily Sales") +
  theme_minimal() # add label and theme

```


Graph 1.6 confirms a negative correlation between total reviews and the number of books sold overall (indicated by the black dotted line). This correlation is most pronounced in YA fiction and least in non-fiction, suggesting that daily sales of YA fiction books may be notably influenced by factors such as pricing.

## Correlationship between variables using NHST approach

```{r}

 # using rcorr to compute correlation and p value

variables_of_interest <- book_sales_revised[c('avg.review', 'total.reviews', 'daily.sales', 'sale.price')]
 
correlation_matrix <- rcorr(as.matrix(variables_of_interest))

correlation_matrix

```

1. avg.review and total.reviews:

A weak positive correlation (r = 0.09) shows a slight association between average review scores and total review counts.
The p-value < 0.05 (very small no.) indicates that this correlation is statistically significant .

2. avg.review and daily.sales:

A negligible negative correlation (r = -0.02) shows a very weak relation between average review scores and daily book sales.
The p-value = 0.6887 indicates that this correlation is not statistically significant.

3. total.reviews and daily.sales:

A moderate positive correlation (r = 0.67) indicates good association between total review counts and daily book sales.
The p-value < 0.05 (very small no.) suggests a statistically significant correlation.


4. daily.sales and sale.price:

A moderate negative correlation (r = -0.51) indicates good association between daily book sales and book sale prices.
The p-value < 0.05 (very small no.) suggests a statistically significant correlation.


## Sales depend upon their average review and total no. of review

```{r}
# LM model between total review and daily. sales as they have high correlation

Daily.sale.total.review <- lm(daily.sales ~ total.reviews, data=book_sales_revised)
 
summary(Daily.sale.total.review) # summary of lm

cbind(coefficient=coef(Daily.sale.total.review), confint(Daily.sale.total.review)) # CI of lm
```

The intercept of 16.76 represents the estimated daily sales when the total reviews are zero. This value is statistically significant (p < 0.001).

The coefficient represent increase of one additional review is associated with an estimated increase of 0.524670 units in daily sales. This increase is significantly different from zero, t(5954)=16.03
, p<.0001

R-squared value states 44.56% of the variability in daily sales is explained by total reviews.

The coefficient  value of sales on one additional review will range between 0.50 - 0.54


```{r}
# compare daily sales with and without total review

Daily.sale.baseline <- lm(daily.sales~1, data = book_sales_revised)
summary(Daily.sale.baseline)

anova(Daily.sale.baseline,Daily.sale.total.review)

```

The model with the predictor variable (total.reviews) has a lower residual standard error (22.11) compared to the baseline model (29.69), indicating better predictive performance.

The F-statistic of 4785.1 at p value < 0.001 strongly suggest that the model with total.reviews is a significantly better fit than the baseline model.

```{r}
# LM model between total review and daily. sales to interpret relationship statistically

Daily.sale.avg.review <- lm(daily.sales ~ avg.review, data=book_sales_revised)
 
summary(Daily.sale.avg.review)
```
 

The coefficient for avg.review (-1.0468) suggests that, on average, for each one-unit increase in avg.review, daily sales decrease by approximately 1.0468 units. But, this effect is not statistically significant, as indicated by the p-value (0.121).

The F-statistic of 2,402 with p value = 0.1212, indicates that the model may not be a significant improvement over a model with no predictors. 
 
 

```{r} 

# analysis more of effect of average review and total review on daily sales


Daily.sales.total.review.add <- lm(daily.sales ~ total.reviews+avg.review, data=book_sales_revised)

summary(Daily.sales.total.review.add)

confint(Daily.sales.total.review.add)

vif(Daily.sales.total.review.add)

test_daily.sales_data <- tibble(total.reviews = rep(c(100,170,240), 3),
                      avg.review = c(rep(3, 3), rep(4, 3), rep(5, 3)))

test_daily.sales_data <- mutate(test_daily.sales_data,
                         daily.sales = predict(Daily.sales.total.review.add,test_daily.sales_data ))

grid.arrange(ggplot(test_daily.sales_data) + geom_line(aes(x = total.reviews, y = daily.sales, colour = as.factor(avg.review))) + labs(colour = "avg review score") + ylab("Predicted daily sales")+xlab("total reviews"), 
          ggplot(test_daily.sales_data) + geom_line(aes(x = avg.review, y = daily.sales, colour = as.factor(total.reviews))) + labs(colour = "total reviews") + ylab("Predicted daily sales") + xlab("average review score"),
          ncol = 2) 

# interaction model to understand interaction term

Daily.sales.total.review.intr <- lm(daily.sales ~ total.reviews*avg.review, data=book_sales_revised)
 
summary(Daily.sales.total.review.intr)

vif(Daily.sales.total.review.intr)
 
```

The Model presenting total review + average review shows that there is a significant positive effect of total review upon daily sales (t(5953)=70.070, p<.001), with every 1  increase in total review predicting an average increase of 0.53  on the daily sales (CI = [0.51, 0.55]); and a negative effect of reading comprehension (t(5953)=-8.489, p<.001), with every 1 point increase in average review score predicting an average decrease of 4.26 on daily sales (CI = [3.28, 5.25]).
They have low VIF scores (approximately 1.008424). This suggests that there is no issue of multicollinearity between these two predictors which enhances the interpretability and stability of model.

We have created a test data frame and predicted daily sales to understand more the interpret ability of relationship and effect of average review and total review on daily sales. It is evident that there is a positive relationship and as the no. of review increase daily sales is also increasing but as the average score increases the daily sales is slightly reducing but overall, the impact of total review seems to have more impact on daily sales.

The Model presenting total review * average review shows that there is a significant main effect of total review and average review sales but The VIF values for total.reviews, avg.review, and the interaction term are 18.836993, 3.819724, and 22.934030, respectively. The elevated VIF for the interaction term indicates a potential issue of multicollinearity.

Given these results, it seems that the multicollinearity issue is more pronounced in the interaction model. This suggests a form of structural multicollinearity, as the introduction of the interaction term has led to higher correlations among the predictors. We can get rid of structural multicollinearity by re-scaling the data and normalising around zero.
 
## Effect of sales price on daily sales of books

```{r}
# Linear model on sales price and daily sales using estimation and NHST

daily.sales.sale.price <- lm(daily.sales~sale.price, data=book_sales_revised)
summary(daily.sales.sale.price)

```
The Model shows that there is a significant negative effect of sales price upon daily sales (t(5954)=-46.04, p<.001), with every 1 pound decrease in sales price predicting an average increase of 3.93  on the daily sales. This model explains 26% variability of daily sales.

```{r}
# NHST approach

anova(daily.sales.sale.price)

daily.sales.sale.price.1 <- lm(daily.sales~1, data = book_sales_revised)
summary(daily.sales.sale.price.1)



# Compare model with or without sales price

anova(daily.sales.sale.price.1, daily.sales.sale.price)
```

The model with sales price has F(5954) = 2119.4 , p value < 0.001 which suggests that including sales price in model is highly significant

To further check the significance, we compared the model with and without sales price using anova. The low p-value (< 2.2e-16) indicates that the improvement is statistically significant, and you have evidence to reject the null hypothesis that the model with sale.price is not better than the intercept-only model.

```{r}

# estimation approach

(  daily.sales.sale.price.emm <- emmeans(daily.sales.sale.price, ~sale.price)  )

```

The emmeans explains that at a sales price of 10.3 the daily sales is estimated to be 86.3 [85.7 - 87] at 95% confidence interval. The narrow confidence interval suggests a relatively precise estimate of the mean at that level.

## Effect of sales price on daily sales of books across genres



```{r}

daily.sales.genre <- lm(daily.sales~genre, data=book_sales_revised)
summary(daily.sales.genre)
(  daily.sales.genre.emm <- emmeans(daily.sales.genre, ~genre)  )


# graphically understanding the relationship

ggplot(book_sales_revised, aes(x = genre, y = daily.sales)) +
  geom_violin() + stat_summary(fun = mean, geom = "point", shape = 20, size = 5, color = "red") +
    labs(title = "2.8 Daily Sales by Genre",
       x = "Genre",
       y = "Daily Sales") +
  theme_minimal()

```

The intercept explains estimated sales of adult_fiction is 82.38 [81.4 - 83.3] at t(5953) = 170.36, p < 0.001 and intervals are at 95% Confidence level and SE of 0.484.

The coefficient for "non_fiction" is 30.75 which states that YA-fiction books are sold more than adult fiction by 30.75 at t(5953) = 44.97, p < 0.001, i.e 113.1 [112.2 - 114.1] at 95% Confidence level and SE of 0.484.

The coefficient for "YA_fiction" is - 18.83 which states that non -fictional books are sold less than adult fiction by 18.83 at t(5953) = -27.56, p < 0.001, i.e 63.6 [62.6 - 64.5] at 95% Confidence level and SE of 0.483.

The model seems to have a reasonably low residual standard error of 21.54, indicating that the model explains a significant portion of the variability in daily sales.

The multiple R-squared is 0.474, suggesting that 47.4% of the variability in daily sales is explained by the predictor 'genre' in the model.

We can validate the above model results graphically as the average sales(mean) in most in YA_fiction and seems like a normal distribution. The average sales of non- fiction is least and width of plot shows more density compared to YA_fiction.

```{r}
# interaction model between genre and sales price

daily.sales.genre.price <- lm(daily.sales~genre*sale.price, data=book_sales_revised)
summary(daily.sales.genre.price)
(  daily.sales.genre.price.emm <- emmeans(daily.sales.genre.price, ~genre)  )

```

The interaction term genrenon_fiction:sale.price states that effect of sales price differ from adult fiction to non_fiction by 0.5128 having t(5950) = 1.520, p = 0.1285 . The high p value is not statistically significant, indicating caution in interpreting this interaction.

The interaction term genreYA_fiction:sale.price states that effect of sales price differ from adult fiction to YA_fiction by -2.772 having t(5950) = -8.147, p < 0.001  which states that YA_fiction experience more reduction in daily sales with 1 unit increase of sales price.

The emmeans function explains the sales interval for each genre at 95% confidence interval

```{r}
anova(daily.sales.genre, daily.sales.genre.price)

```

The F score of 67.761 with a p value < 0.001 is sufficient to reject null hypothesis and interaction significantly imroved model ability to explain variability in daily sales.

```{r}

# analysis of difference in mean of above models using graph

daily.sales.genre.emm.summary <- summary(daily.sales.genre.emm)
daily.sales.genre.emm.summary$model <- "Only Genre"
daily.sales.genre.price.emm.summary <- summary(daily.sales.genre.price.emm)
daily.sales.genre.price.emm.summary$model <- "Genre With Sales price"
daily.sales.genre.both.model.summary <- rbind(daily.sales.genre.emm.summary, daily.sales.genre.price.emm.summary)

ggplot(daily.sales.genre.both.model.summary, aes(x=genre, y=emmean, ymin=lower.CL, ymax=upper.CL, col=model)) + geom_point() + geom_linerange() + labs(x="Genre", y="Daily Sales", col="Model", title ="2.9 Difference in Mean of Model")
# the graph indicates that daily sales is impacted by the sales price but is also different across genres. 
```

We can interpret from graph that both models show different estimated mean values with genre and with genre and sales price. The difference is highest in YA_fiction where sales price has most impacted the mean compared to any other genre.

Hence, We can say that daily sales is impacted by sales price differently across genres.

## Conclusion

1. We can emphasis on gathering more reviews to enhance daily sales performance.
2. There is a limited influence of average review score on daily sales.
3. We can make pricing strategy genre wise , with special attention to responsiveness of YA_fiction to sales price.


# Reference

www.stackoverflow.com

R for Data Science Garrett Grolemund (Author), Hadley Wickham (Author)

www.datacamp.com

Lecture and workshop resources

Article Ref 1: https://londonist.com/london/transport/no-santander-cycle-hire-on-weekend-of-10-11-september 

Article Ref 2: https://discerningcyclist.com/bike-boom-numbers-statistics/



